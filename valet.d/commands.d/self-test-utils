#!/usr/bin/env bash
set -Eeu -o pipefail
# author: github.com/jcaillon
# description: Utility functions for the self-test script.

#===============================================================
# >>> Functions that can be used in the test scripts
#===============================================================

# shellcheck source=../lib-io
source io

# ## test::commentTest
#
# Call this function to add a paragraph in the report file.
#
# - $1: **comment** _as string_:
#       the text to add in the report file
#
# ```bash
# test::commentTest "This is a comment."
# ```
function test::commentTest() {
  printf "%s\n\n" "${1:-}" >>"${_TEST_REPORT_FILE}"
}

# ## test::endTest
#
# Call this function after each test to write the test results to the report file.
# This create a new H3 section in the report file with the test description and the exit code.
#
# - $1: **title** _as string_:
#       the title of the test
# - $2: **exit code** _as int_:
#       the exit code of the test
# - $3: comment _as string_:
#       (optional) a text to explain what is being tested
#       (defaults to "")
#
# ```bash
# test::endTest "Testing something" $?
# ```
function test::endTest() {
  local testTitle="${1:-}"
  local exitCode="${2:-}"
  local testDescription="${3:-}"

  {
    # write the test title
    printf "%s\n\n" "### ${testTitle:-test}"

    # write the test description if any
    if [[ -n "${testDescription}" ]]; then
      printf "%s\n\n" "${testDescription}"
    fi

    # write the exit code
    printf "%s\n\n" "Exit code: \`${exitCode}\`"

    # write the standard output if any
    if [[ -s "${_TEST_STANDARD_OUTPUT_FILE}" ]]; then
      printf "%s\n\n%s\n" "**Standard** output:" "\`\`\`plaintext"
      selfTestUtils_echoFileSubstitutingPath "${_TEST_STANDARD_OUTPUT_FILE}"
      printf "\n%s\n\n" "\`\`\`"
    fi

    # write the error output if any
    if [[ -s "${_TEST_STANDARD_ERROR_FILE}" ]]; then
      printf "%s\n\n%s\n" "**Error** output:" "\`\`\`log"
      selfTestUtils_echoFileSubstitutingPath "${_TEST_STANDARD_ERROR_FILE}"
      printf "\n%s\n\n" "\`\`\`"
    fi

  } >>"${_TEST_REPORT_FILE}"

  # reset the standard output and error output files
  : >"${_TEST_STANDARD_OUTPUT_FILE}"
  : >"${_TEST_STANDARD_ERROR_FILE}"
}

# Replaces the line numbers from an error log in the given file with XXX.
#
# - $1: **file path** _as string_:
#       the path of the file in which the output is stored
#
# ```bash
# test::echoFileWithLineNumberSubstitution myFile.log
# ```
function test::echoFileWithLineNumberSubstitution() {
  local file="${1}"
  local line
  local IFS=$'\n'
  while read -rd $'\n' line || [[ -n ${line:-} ]]; do
    if [[ ${line} =~ :[0-9]{1,4}$ ]]; then
      line="${line/%:[[:digit:]]/:XXX}"
      line="${line/%:[[:digit:]][[:digit:]]/:XXX}"
      line="${line/%:[[:digit:]][[:digit:]][[:digit:]]/:XXX}"
      line="${line/%:[[:digit:]][[:digit:]][[:digit:]][[:digit:]]/:XXX}"
    fi
    printf '%s\n' "${line}"
  done <"${file}"
}

#===============================================================
# >>> Internal tests functions
#===============================================================

function selfTestUtils_rebuildCommands() {
  local originalLogLevel
  log::getLevel && originalLogLevel="${RETURNED_VALUE}"
  log::setLevel warning true
  core::sourceFunction selfBuild
  selfBuild "$@"
  log::setLevel "${originalLogLevel}" true
}

# Run all the test suites in the given directory.
# A test suite is a folder that contains a test.sh file.
# This test.sh allows to run multiple tests.
#
# $1: the directory containing the test suites
#
# Returns:
#   the number of test suites run (increments the global variable NB_TEST_SUITES)
#   the number of failed test suites (increments the global variable NB_FAILED_TEST_SUITES)
#
# Usage:
#   selfTestUtils_runTestSuites "${GLOBAL_VALET_HOME}/tests.d"
function selfTestUtils_runTestSuites() {
  local testSuiteDirectory="${1}"

  # run a custom user script before the tests if it exists
  selfTestUtils_runHookScript "${testSuiteDirectory}/before-tests"

  local -a testSuitePids=()

  # for each test file in the test directory
  local testDirectory testDirectoryName testScript
  for testDirectory in "${testSuiteDirectory}/"*; do
    testDirectoryName="${testDirectory##*/}"

    # skip if not a directory or hidden dir
    if [[ ! -d ${testDirectory} || ${testDirectoryName} == "."* ]]; then continue; fi

    # skip if the test directory does not match the include pattern
    if [[ -n ${TEST_INCLUDE_PATTERN:-} && ! (${testDirectory} =~ ${TEST_INCLUDE_PATTERN}) ]]; then
      log::debug "Skipping test ⌜${testDirectory}⌝ because it does not match the include pattern."
      continue
    fi
    # skip if the test directory matches the exclude pattern
    if [[ -n ${TEST_EXCLUDE_PATTERN:-} && ${testDirectory} =~ ${TEST_EXCLUDE_PATTERN} ]]; then
      log::debug "Skipping test ⌜${testDirectory}⌝ because it matches the exclude pattern."
      continue
    fi

    log::debug "Starting test suite in directory ⌜${testDirectory}⌝."

    # run the test suite in the background
    # we want shopt -u -o monitor to avoid the "[x]   Done" message when a background process is finished
    shopt -u -o monitor
    (selfTestUtils_runTestSuite "${testDirectory}"; ) &
    testSuitePids+=("$!")
  done

  # wait for all the test suites to finish
  local -i failedTestSuites=0
  local -i exitCode
  local -i idx
  local pid
  for idx in "${!testSuitePids[@]}"; do
    pid="${testSuitePids[idx]}"
    exitCode=0
    # interactive::updateProgress $((idx * 100 / ${#testSuitePids[@]})) "Waiting for test suite with PID ${pid}."
    wait "${pid}" || exitCode=$?
    if (( exitCode != 0 )); then
      failedTestSuites+=1
    fi
  done

  # interactive::stopProgress

  shopt -s -o monitor

  # run a custom user script after the tests if it exists
  selfTestUtils_runHookScript "${testSuiteDirectory}/after-tests"

  NB_TEST_SUITES=$((NB_TEST_SUITES + ${#testSuitePids[@]}))
  NB_FAILED_TEST_SUITES=$((NB_FAILED_TEST_SUITES + failedTestSuites))
}

function selfTestUtils_runTestSuite() {
  local testSuiteDirectory="${1}"

  _TEST_SUITE_NAME="${testSuiteDirectory##*/}"

  _TEST_STANDARD_OUTPUT_FILE="${GLOBAL_TEMPORARY_IN_MEM_PREFIX}${BASHPID}.valet-test-stdout-${_TEST_SUITE_NAME}"
  _TEST_STANDARD_ERROR_FILE="${GLOBAL_TEMPORARY_IN_MEM_PREFIX}${BASHPID}.valet-test-stderr-${_TEST_SUITE_NAME}"
  _TEST_REPORT_FILE="${GLOBAL_TEMPORARY_IN_MEM_PREFIX}${BASHPID}.valet-test-report-${_TEST_SUITE_NAME}"

  # reregister the traps, remove cleanup we only want to do it after all test suites
  main::registrerTraps
  # setup the temp locations for this test suite
  _TEST_BASE_TEMPORARY_DIRECTORY="${TMPDIR:-/tmp}/valet-tests-${BASHPID}-${_TEST_SUITE_NAME}"

  log::debug "Executing test suite ⌜${testDirectoryName}⌝."

  # run a custom user script before the test suite if it exists
  selfTestUtils_runHookScript "${testSuiteDirectory}/before-each-test-suite"

  # write the test suite title
  printf "%s\n\n" "# Test suite ${testDirectoryName}" >"${_TEST_REPORT_FILE}"

  local testLogLines="Executed test suite ⌜${testDirectoryName}⌝."$'\n'

  # for each .sh script in the test directory, run the test
  for testScript in "${testDirectory}"/*.sh; do
    # skip if not a file
    if [[ ! -f "${testScript}" ]]; then
      continue
    fi

    _TEST_SUITE_SCRIPT_NAME="${testScript##*/}"
    testLogLines+="                    ├── ⌜${_TEST_SUITE_SCRIPT_NAME}⌝."$'\n'

    selfTestUtils_runTest "${testDirectory}" "${testScript}"
  done

  # run a custom user script after the test suite if it exists
  selfTestUtils_runHookScript "${testSuiteDirectory}/after-each-test-suite"

  GLOBAL_ERROR_DISPLAYED=1
  selfTestUtils_compareWithApproved "${testDirectory}" "${_TEST_REPORT_FILE}" "${testLogLines}" || exit 1
  exit 0
}

function selfTestUtils_runTest() {
  local testDirectory="${1}"
  local testScript="${2}"

  # make sure we set the options to fail on error, pipe erors and on unset variables
  set -Eeu -o pipefail

  # set a simplified log print function to have consistent results in tests
  selfTestUtils_evalSimplifiedImportantVariables

  # reset the temporary location (to have consistency when using io::createTempDirectory for example)
  if [[ -d ${_TEST_BASE_TEMPORARY_DIRECTORY} ]]; then
    rm -Rf "${_TEST_BASE_TEMPORARY_DIRECTORY}"
  fi
  TMPDIR="${_TEST_BASE_TEMPORARY_DIRECTORY}"
  VALET_CONFIG_WORK_FILES_DIRECTORY="${_TEST_BASE_TEMPORARY_DIRECTORY}"
  io::setupTempFileGlobalVariable
  mkdir -p "${_TEST_BASE_TEMPORARY_DIRECTORY}"
  TEMPORARY_FILE_NUMBER=0
  TEMPORARY_DIRECTORY_NUMBER=0
  GLOBAL_TEST_TEMP_FILE="${GLOBAL_TEMPORARY_IN_MEM_PREFIX}${BASHPID}.valet-test-tempfile"

  # redirect the standard output and error output to files
  selfTestUtils_SetFdRedirection

  # make sure that, if we exit in the test, we catch that and explain to the user
  # why this should not happen
  eval "function onExitTest() { selfTestUtils_onExitTestInternal "\$@"; }"

  # used in selfTestUtils_echoFileSubstitutingPath to replace this path with .
  CURRENT_DIRECTORY="${PWD}"

  # write the test script name
  printf "%s\n\n" "## Test script ${_TEST_SUITE_SCRIPT_NAME%.sh}" >>"${_TEST_REPORT_FILE}"

  # run the test
  pushd "${testDirectory}" >/dev/null
  # shellcheck disable=SC1090
  builtin source "${testScript}"
  popd >/dev/null

  # remove the onExitTest function, we can exit safely now
  unset -f onExitTest

  selfTestUtils_resetFdRedirection

  # restore the important variables
  selfTestUtils_evalOriginalImportantVariables

  # test if the user forgot to call test::endTest
  if [[ -s "${_TEST_STANDARD_OUTPUT_FILE}" || -s "${_TEST_STANDARD_ERROR_FILE}" ]]; then
    selfTestUtils_displayTestSuiteOutputs
    core::fail "The test ⌜${_TEST_SUITE_NAME}⌝ → ⌜${_TEST_SUITE_SCRIPT_NAME}⌝ did not call test::endTest OR it had outputs after the last test::endTest call."$'\n'"Make sure to conclude all tests by calling the test::endTest function."
  fi
}

function selfTestUtils_runHookScript() {
  # run a custom script if it exists
  if [[ -f "${1}" ]]; then
    log::info "Running (source) ${1##*/} script."
    # shellcheck disable=SC1091
    # shellcheck disable=SC1090
    source "${1}"
  else
    log::debug "No script found for ${1##*/}."
  fi
}

function selfTestUtils_compareWithApproved() {
  local testDirectory="${1}"
  local receivedFileToCopy="${2}"
  local testLogLines="${3}"

  local exitCode approvedFile receivedFile

  approvedFile="${testDirectory}/results.approved.md"
  receivedFile="${testDirectory}/results.received.md"

  if [[ ! -f "${approvedFile}" ]]; then
    : >"${approvedFile}"
  fi

  local command="${_TEST_DIFF_COMMAND//%APPROVED_FILE%/"${approvedFile}"}"
  command="${command//%RECEIVED_FILE%/"${receivedFileToCopy}"}"

  if [[ -n ${VALET_CONFIG_TEST_REPORT_FILE_MODE:-} ]]; then
    chmod "${VALET_CONFIG_TEST_REPORT_FILE_MODE}" "${receivedFileToCopy}"
  fi

  if ${command} 1>&2; then
    log::success "${testLogLines}→ test suite has passed."
    if [[ -f "${receivedFile}" ]]; then
      rm -f "${receivedFile}" 1>/dev/null
    fi
    return 0
  else
    log::printString "${receivedFile} is different than ${approvedFile}."
    log::error "${testLogLines}→ test suite KO, received file differs from the approved file."
  fi

  # if the option is activated, we approve the received file
  if [[ ${TEST_AUTO_APPROVE:-false} == "true" ]]; then
    log::info "${testLogLines}→ test suite KO but auto-approving."
    cp -f "${receivedFileToCopy}" "${approvedFile}"
    if [[ -n ${VALET_CONFIG_TEST_REPORT_FILE_MODE:-} ]]; then
      chmod "${VALET_CONFIG_TEST_REPORT_FILE_MODE}" "${approvedFile}"
    fi
    if [[ -f "${receivedFile}" ]]; then
      rm -f "${receivedFile}" 1>/dev/null
    fi
  else
    cp -f "${receivedFileToCopy}" "${receivedFile}"
    if [[ -n ${VALET_CONFIG_TEST_REPORT_FILE_MODE:-} ]]; then
      chmod "${VALET_CONFIG_TEST_REPORT_FILE_MODE}" "${approvedFile}"
    fi
  fi

  return 1
}

# Allows to explicitly warn the user that a test made Valet exit (it should not).
# Give guidance on what to do to fix this.
function selfTestUtils_onExitTestInternal() {
  local rc="${1:-}"

  selfTestUtils_resetFdRedirection

  # restore the important variables
  selfTestUtils_evalOriginalImportantVariables

  selfTestUtils_displayTestSuiteOutputs

  log::error "The program has exit during the test ⌜${_TEST_SUITE_NAME}⌝ → ⌜${_TEST_SUITE_SCRIPT_NAME}⌝ with code ⌜${rc}⌝."$'\n'"If you expect the tested function/program to exit or fail, then run it in a subshell and capture the exit code like that:"$'\n'"(myFunctionThatFails) || exitCode=\$?"

  GLOBAL_ERROR_DISPLAYED=1
}

function selfTestUtils_displayTestSuiteOutputs() {
  if [[ -s "${_TEST_REPORT_FILE}" ]]; then
    log::errorTrace "Test suite report for ⌜${_TEST_SUITE_NAME}⌝:"
    log::printFile "${_TEST_REPORT_FILE}"
  else
    log::errorTrace "Empty report for the test suite ⌜${_TEST_SUITE_NAME}⌝."
  fi

  if [[ -s "${_TEST_STANDARD_OUTPUT_FILE}" ]]; then
    log::errorTrace "Test suite standard output for ⌜${_TEST_SUITE_NAME}⌝:"
    io::readFile "${_TEST_STANDARD_OUTPUT_FILE}"
    log::printFileString "${RETURNED_VALUE}"
  else
    log::errorTrace "Empty standard output for the test suite ⌜${_TEST_SUITE_NAME}⌝."
  fi

  if [[ -s "${_TEST_STANDARD_ERROR_FILE}" ]]; then
    log::errorTrace "Test suite error output for ⌜${_TEST_SUITE_NAME}⌝:"
    io::readFile "${_TEST_STANDARD_ERROR_FILE}"
    log::printFileString "${RETURNED_VALUE}"
  else
    log::errorTrace "Empty error output for the test suite ⌜${_TEST_SUITE_NAME}⌝."
  fi
}

function cleanUpTest() {
  # remove the temporary directory
  if [[ -n ${_TEST_BASE_TEMPORARY_DIRECTORY:-} && -d ${_TEST_BASE_TEMPORARY_DIRECTORY:-} ]]; then
    rm -Rf "${_TEST_BASE_TEMPORARY_DIRECTORY}"
  fi
}

# After this function call, everything written to stdout and stderr will be redirected
# to the test output files.
# Call selfTestUtils_resetFdRedirection to reset the redirection.
function selfTestUtils_SetFdRedirection() {
  # redirect the standard output and error output to files
  exec 3>&1 1>"${_TEST_STANDARD_OUTPUT_FILE}"
  exec 4>&2 2>"${_TEST_STANDARD_ERROR_FILE}"
}

# Restores the standard output and error output.
# Call this function after selfTestUtils_SetFdRedirection.
function selfTestUtils_resetFdRedirection() {
  # reset the standard output and error output
  exec 1>&3 3>&-
  exec 2>&4 4>&-
}

# This function creates strings that can be evaluated:
# - to reset all valet config and global variables to their original values,
#   as well as the log line function.
# - valet config and global variables to simpler values
#   in order to have consistent results in the tests.
#   As well as a simplified log line function.
function selfTestUtils_setEvalVariables() {
  io::invoke declare -p ${!VALET_CONFIG_*} ${!GLOBAL_*}
  ORIGINAL_IMPORTANT_VARIABLES="${GLOBAL_LOG_PRINT_FUNCTION}"$'\n'"${RETURNED_VALUE//declare -? /}"

  # unset all VALET_ variables to not have the user variables interfere with the tests
  SIMPLIFIED_IMPORTANT_VARIABLES="
  unset -v ${!VALET_*}

  export VALET_CONFIG_ENABLE_COLORS=false
  export VALET_CONFIG_ENABLE_NERDFONT_ICONS=false
  export VALET_CONFIG_DISABLE_LOG_TIME=true
  export VALET_CONFIG_DISABLE_LOG_WRAP=true
  export VALET_CONFIG_ENABLE_LOG_TIMESTAMP=false
  export VALET_CONFIG_LOG_COLUMNS=9999

  export VALET_CONFIG_ICON_ERROR=IE
  export VALET_CONFIG_ICON_WARNING=IW
  export VALET_CONFIG_ICON_SUCCESS=IS
  export VALET_CONFIG_ICON_INFO=II
  export VALET_CONFIG_ICON_DEBUG=ID
  export VALET_CONFIG_ICON_EXIT=IX
  export VALET_CONFIG_ICON_STOPPED=IP
  export VALET_CONFIG_ICON_KILLED=IK

  export VALET_CONFIG_COLOR_DEFAULT=CDE
  export VALET_CONFIG_COLOR_DEBUG=CDB
  export VALET_CONFIG_COLOR_INFO=CIN
  export VALET_CONFIG_COLOR_WARNING=CWA
  export VALET_CONFIG_COLOR_SUCCESS=CSU
  export VALET_CONFIG_COLOR_ERROR=CER
  export VALET_CONFIG_COLOR_TIMESTAMP=CTI
  export VALET_CONFIG_COLOR_HIGHLIGHT=CHI
  export VALET_CONFIG_COLOR_TITLE=CTT
  export VALET_CONFIG_COLOR_OPTION=COP
  export VALET_CONFIG_COLOR_ARGUMENT=CAR
  export VALET_CONFIG_COLOR_COMMAND=CCO
  export VALET_CONFIG_COLOR_ACTIVE_BUTTON=CAB
  export VALET_CONFIG_COLOR_UNACTIVE_BUTTON=CUB

  export VALET_CONFIG_DOT_ENV_SCRIPT=false

  export VALET_CONFIG_DISABLE_PROGRESS=true

  export GLOBAL_COLUMNS=9999
  "

  eval "${SIMPLIFIED_IMPORTANT_VARIABLES}"
  log::createPrintFunction
  SIMPLIFIED_IMPORTANT_VARIABLES+="
  ${GLOBAL_LOG_PRINT_FUNCTION}
  "

  selfTestUtils_evalOriginalImportantVariables
}

function selfTestUtils_evalSimplifiedImportantVariables() {
  unset -v ${!VALET_CONFIG_*} ${!GLOBAL_*}
  eval "${ORIGINAL_IMPORTANT_VARIABLES}"$'\n'"${SIMPLIFIED_IMPORTANT_VARIABLES}"
}

function selfTestUtils_evalOriginalImportantVariables() {
  unset -v ${!VALET_CONFIG_*} ${!GLOBAL_*}
  eval "${ORIGINAL_IMPORTANT_VARIABLES}"
}

# This function is used to print the content of a file with some substitutions.
# The substitutions are:
# - replace the GLOBAL_VALET_HOME with $GLOBAL_VALET_HOME
# - replace the current test directory with a dot
# - replace a line with ${TMPDIR}/valet-*/ (temp directory) by /valet/
#
# This allows to have consistent results accross different execution environments for the tests.
#
# Usage:
#   myCommandThatProducesLinesWithValetDirectoryPath 2> "${GLOBAL_TEST_TEMP_FILE}"
#   selfTestUtils_echoFileSubstitutingPath "${GLOBAL_TEST_TEMP_FILE}" 1>&2
function selfTestUtils_echoFileSubstitutingPath() {
  local file="${1}"
  local line
  local IFS=$'\n'
  local -i firstLine=1
  while read -rd $'\n' line || [[ -n ${line:-} ]]; do
    if [[ firstLine -eq 1 ]]; then
      firstLine=0
    else
      echo
    fi
    line="${line//${GLOBAL_VALET_HOME}/\$GLOBAL_VALET_HOME}"
    line="${line//${CURRENT_DIRECTORY}/.}"
    line="${line//${GLOBAL_TEMPORARY_PREFIX}*.valet/\/tmp/valet}"
    line="${line//${GLOBAL_TEMPORARY_IN_MEM_PREFIX}*.valet/\/tmp/valet}"
    if [[ ${line} =~ "after "[0-9]{1,}s.$ ]]; then
      line="${line/%after */after Xs.}"
    fi
    printf '%s' "${line}"
  done <"${file}"
}

function selfTestUtils_setupDiffCommand() {
  _TEST_DIFF_COMMAND="${VALET_CONFIG_TEST_DIFF_COMMAND:-}"
  if [[ -z ${_TEST_DIFF_COMMAND} ]]; then
    if command -v delta &>/dev/null; then
      log::info "Using delta as diff command."
      _TEST_DIFF_COMMAND="delta --paging=never --no-gitconfig --line-numbers --side-by-side %APPROVED_FILE% %RECEIVED_FILE%"
      if [[ -z ${VALET_CONFIG_TEST_REPORT_FILE_MODE:-} ]]; then
        # delta compares the file modes, so we need to match them
        VALET_CONFIG_TEST_REPORT_FILE_MODE="644"
      fi
    elif command -v diff &>/dev/null; then
      log::info "Using diff as diff command."
      _TEST_DIFF_COMMAND="diff --color -u %APPROVED_FILE% %RECEIVED_FILE%"
    elif command -v cmp &>/dev/null; then
      log::warning "Using cmp as diff command, consider setting up a diff tool using the VALET_CONFIG_TEST_DIFF_COMMAND config variable."
      _TEST_DIFF_COMMAND="cmp %APPROVED_FILE% %RECEIVED_FILE%"
    else
      log::warning "Using internal comparison function, consider setting up a diff tool using the VALET_CONFIG_TEST_DIFF_COMMAND config variable."
      _TEST_DIFF_COMMAND="selfTestUtils_internalCompare %APPROVED_FILE% %RECEIVED_FILE%"
    fi
  else
    string::cutField "${_TEST_DIFF_COMMAND}" 0 " "
    local diffExecutable="${RETURNED_VALUE}"
    if ! command -v "${diffExecutable}" &>/dev/null; then
      log::warning "The user diff command ⌜${diffExecutable}⌝ set with VALET_CONFIG_TEST_DIFF_COMMAND is not available, using the internal comparison function."
    fi
  fi
}

# This function allows to compare the content two files.
# It mimics the diff command.
#
# - $1: the first file to compare
# - $2: the second file to compare
#
# Returns:
#
# -$?: 0 if the files are the same, 1 otherwise
#
# ```bash
# selfTestUtils_internalCompare file1 file2
# ```
function selfTestUtils_internalCompare() {
  local file1="${1}"
  local file2="${2}"

  local content1 content2
  io::readFile "${file1}"
  content1="${RETURNED_VALUE}"
  io::readFile "${file2}"
  content2="${RETURNED_VALUE}"

  if [[ "${content1}" != "${content2}" ]]; then
    return 1
  fi
  return 0
}
