#!/usr/bin/env bash
set -Eeu -o pipefail
# author: github.com/jcaillon
# description: Utility functions for the self-test script.

#===============================================================
# >>> Functions that can be used in the test scripts
#===============================================================

# shellcheck source=../lib-io
source io

# ## test::commentTest
#
# Call this function to add a paragraph in the report file.
#
# - $1: the text to add in the report file
#
# ```bash
# test::commentTest "This is a comment."
# ```
function test::commentTest() {
  printf "%s\n\n" "${1:-}" >>"${_TEST_REPORT_FILE}"
}

# ## test::endTest
#
# Call this function after each test to write the test results to the report file.
# This create a new H3 section in the report file with the test description and the exit code.
#
# - $1: the title of the test
# - $2: the exit code of the test
# - $3: (optional) a text to explain what is being tested
#
# ```bash
#   test::endTest "Testing something" $?
# ```
function test::endTest() {
  local testTitle="${1:-}"
  local exitCode="${2:-}"
  local testDescription="${3:-}"

  local fdRedirectionReset=false
  if [[ ${ORIGINAL_GLOBAL_LOG_LEVEL} == "debug" ]]; then
    resetFdRedirection
    fdRedirectionReset=true
    log::debug "Ended test ⌜${testTitle}⌝ with exit code ⌜${exitCode}⌝."
  fi

  {
    # write the test title
    printf "%s\n\n" "### ${testTitle:-test}"

    # write the test description if any
    if [[ -n "${testDescription}" ]]; then
      printf "%s\n\n" "${testDescription}"
    fi

    # write the exit code
    printf "%s\n\n" "Exit code: \`${exitCode}\`"

    # write the standard output if any
    if [[ -s "${_TEST_STANDARD_OUTPUT_FILE}" ]]; then
      printf "%s\n\n%s\n" "**Standard** output:" "\`\`\`plaintext"
      echoFileSubstitutingPath "${_TEST_STANDARD_OUTPUT_FILE}"
      printf "\n%s\n\n" "\`\`\`"
    fi

    # write the error output if any
    if [[ -s "${_TEST_STANDARD_ERROR_FILE}" ]]; then
      printf "%s\n\n%s\n" "**Error** output:" "\`\`\`log"
      echoFileSubstitutingPath "${_TEST_STANDARD_ERROR_FILE}"
      printf "\n%s\n\n" "\`\`\`"
    fi

  } >>"${_TEST_REPORT_FILE}"

  # reset the standard output and error output files
  : >"${_TEST_STANDARD_OUTPUT_FILE}"
  : >"${_TEST_STANDARD_ERROR_FILE}"

  if [[ ${fdRedirectionReset} == "true" ]]; then
    setFdRedirection
  fi
}

function test::echoFileWithLineNumberSubstitution() {
  local file="${1}"
  local line
  local IFS=$'\n'
  while read -rd $'\n' line || [[ -n ${line:-} ]]; do
    if [[ ${line} =~ :[0-9]{1,4}$ ]]; then
      line="${line/%:[[:digit:]]/:XXX}"
      line="${line/%:[[:digit:]][[:digit:]]/:XXX}"
      line="${line/%:[[:digit:]][[:digit:]][[:digit:]]/:XXX}"
      line="${line/%:[[:digit:]][[:digit:]][[:digit:]][[:digit:]]/:XXX}"
    fi
    printf '%s\n' "${line}"
  done <"${file}"
}

#===============================================================
# >>> Internal tests functions
#===============================================================

# Allows to run the core tests of Valet (tests.d directory in the repo) as
# well as the examples (examples.d in the repo).
function runCoreTests() {
  if [[ ! -d "${GLOBAL_VALET_HOME}/tests.d" ]]; then
    core::fail "The valet core tests directory ⌜${GLOBAL_VALET_HOME}/tests.d⌝ does not exist, cannot run core tests."
  fi

  # we need to rebuild the commands for the core commands only
  rebuildCommands "" ""

  # we should always run the test suite from the valet home directory to have consistent paths
  # in the report files
  pushd "${GLOBAL_VALET_HOME}" 1>/dev/null

  log::info "Running all test suites in directory ⌜${GLOBAL_VALET_HOME}/tests.d⌝."
  runTestSuites "${GLOBAL_VALET_HOME}/tests.d"

  # now we can also test the commands in examples.d if the directory is there
  if [[ ! -d "${GLOBAL_VALET_HOME}/examples.d" ]]; then
    log::warning "The valet examples directory ⌜${GLOBAL_VALET_HOME}/examples.d⌝ does not exist, cannot run the tests on the core examples."
  else
    # we need to rebuild the commands for the examples only
    rebuildCommands "${GLOBAL_VALET_HOME}/examples.d" ""

    log::info "Running all test suites in directory ⌜${GLOBAL_VALET_HOME}/examples.d⌝."
    runTestSuites "${GLOBAL_VALET_HOME}/examples.d/showcase/tests.d"
  fi

  popd 1>/dev/null

  # reload the orignal commands
  core::reloadUserCommands
}

function rebuildCommands() {
  local originalLogLevel
  log::getLevel && originalLogLevel="${RETURNED_VALUE}"
  log::setLevel warning true
  core::sourceFunction selfBuild
  if (($# == 1)); then
    selfBuild --user-directory "${1:-}"
  else
    selfBuild --user-directory "${1:-}" --output "${2}"
  fi
  log::setLevel "${originalLogLevel}" true
}

# Run all the test suites in the given directory.
# A test suite is a folder that contains a test.sh file.
# This test.sh allows to run multiple tests.
#
# $1: the directory containing the test suites
#
# Returns:
#   the number of test suites run (increments the global variable NB_TEST_SUITES)
#   the number of failed test suites (increments the global variable NB_FAILED_TEST_SUITES)
#
# Usage:
#   runTestSuites "${GLOBAL_VALET_HOME}/tests.d"
function runTestSuites() {
  local testSuiteDirectory="${1}"

  _TEST_STANDARD_OUTPUT_FILE="${GLOBAL_TEMPORARY_IN_MEM_PREFIX}${BASHPID}.valet-test-stdout"
  _TEST_STANDARD_ERROR_FILE="${GLOBAL_TEMPORARY_IN_MEM_PREFIX}${BASHPID}.valet-test-stderr"
  _TEST_REPORT_FILE="${GLOBAL_TEMPORARY_IN_MEM_PREFIX}${BASHPID}.valet-test-report"
  _TEST_TEMP_FILE="${GLOBAL_TEMPORARY_IN_MEM_PREFIX}${BASHPID}.valet-test-tempfile"

  # save the original important variables so we can restore them after the test suite
  log::getLevel
  ORIGINAL_GLOBAL_LOG_LEVEL="${RETURNED_VALUE}"
  ORIGINAL_GLOBAL_LOG_PRINT_FUNCTION="${GLOBAL_LOG_PRINT_FUNCTION}"
  setOriginalImportantVariables
  setSimplifiedImportantVariables
  resetOriginalImportantVariables

  local -i failedTestSuites nbTestSuites
  failedTestSuites=0
  nbTestSuites=0

  # run a custom user script before the tests if it exists
  runHookScript "${testSuiteDirectory}/before-tests"

  # for each test file in the test directory
  local testDirectory exitCode testDirectoryName testScript
  for testDirectory in "${testSuiteDirectory}/"*; do
    testDirectoryName="${testDirectory##*/}"

    # skip if not a directory or hidden dir
    if [[ ! -d ${testDirectory} || ${testDirectoryName} == "."* ]]; then continue; fi

    # skip if the test directory does not match the include pattern
    if [[ -n ${TEST_INCLUDE_PATTERN:-} && ! (${testDirectory} =~ ${TEST_INCLUDE_PATTERN}) ]]; then
      log::debug "Skipping test ⌜${testDirectory}⌝ because it does not match the include pattern."
      continue
    fi
    # skip if the test directory matches the exclude pattern
    if [[ -n ${TEST_EXCLUDE_PATTERN:-} && ${testDirectory} =~ ${TEST_EXCLUDE_PATTERN} ]]; then
      log::debug "Skipping test ⌜${testDirectory}⌝ because it matches the exclude pattern."
      continue
    fi

    # reset the temp files
    io::cleanupTempFiles

    log::info "Running test suite ⌜${testDirectoryName}⌝."
    log::debug "Test suite directory is ⌜${testDirectory}⌝."

    # run a custom user script before the test suite if it exists
    runHookScript "${testSuiteDirectory}/before-each-test-suite"

    # write the test suite title
    printf "%s\n\n" "# Test suite ${testDirectoryName}" >"${_TEST_REPORT_FILE}"

    # for each .sh script in the test directory, run the test
    for testScript in "${testDirectory}"/*.sh; do
      # skip if not a file
      if [[ ! -f "${testScript}" ]]; then continue; fi

      log::info "Running test       ├── ⌜${testScript##*/}⌝."

      runTest "${testDirectory}" "${testScript}"
    done

    exitCode=0
    compareWithApproved "${testDirectory}" "${_TEST_REPORT_FILE}" || exitCode=$?
    nbTestSuites+=1

    if [[ "${exitCode}" -eq 0 ]]; then
      log::success "Test suite ${testDirectoryName} passed."
    else
      log::error "Test suite ${testDirectoryName} failed."
      failedTestSuites+=1
    fi

    # run a custom user script after the test suite if it exists
    runHookScript "${testSuiteDirectory}/after-each-test-suite"

  done

  # run a custom user script after the tests if it exists
  runHookScript "${testSuiteDirectory}/after-tests"

  NB_TEST_SUITES=$((NB_TEST_SUITES + nbTestSuites))
  NB_FAILED_TEST_SUITES=$((NB_FAILED_TEST_SUITES + failedTestSuites))
}

function runHookScript() {
  # run a custom script if it exists
  if [[ -f "${1}" ]]; then
    log::info "Running (source) ${1##*/} script."
    # shellcheck disable=SC1091
    # shellcheck disable=SC1090
    source "${1}"
  else
    log::debug "No script found for ${1##*/}."
  fi
}

function runTest() {
  local testDirectory="${1}"
  local testScript="${2}"

  # make sure we set the options to fail on error, pipe erors and on unset variables
  set -Eeu -o pipefail

  # set a simplified log print function to have consistent results in tests
  eval "${SIMPLIFIED_IMPORTANT_VARIABLES}"

  # redirect the standard output and error output to files
  setFdRedirection

  # make sure that, if we exit in the test, we catch that and explain to the user
  # why this should not happen
  eval "function onExitTest() { onExitTestInternal "\$@"; }"

  # used in echoFileSubstitutingPath to replace this path with .
  CURRENT_DIRECTORY="${PWD}"

  # write the test script name
  local scriptName="${testScript##*/}"
  scriptName="${scriptName%.sh}"
  printf "%s\n\n" "## Test script ${scriptName}" >>"${_TEST_REPORT_FILE}"

  # run the test
  pushd "${testDirectory}" >/dev/null
  # shellcheck disable=SC1090
  builtin source "${testScript}"
  popd >/dev/null

  # remove the onExitTest function, we can exit safely now
  unset -f onExitTest

  resetFdRedirection

  # restore the important variables
  resetOriginalImportantVariables

  # test if the user forgot to call test::endTest
  if [[ -s "${_TEST_STANDARD_OUTPUT_FILE}" || -s "${_TEST_STANDARD_ERROR_FILE}" ]]; then
    local content1 content2
    io::readFile "${_TEST_STANDARD_OUTPUT_FILE}" && content1="${RETURNED_VALUE}"
    io::readFile "${_TEST_STANDARD_ERROR_FILE}" && content2="${RETURNED_VALUE}"
    core::fail "The test script ⌜${testScript}⌝ did not call test::endTest OR it had outputs after the last test::endTest call."$'\n'"Standard output"$'\n'"⌜${content1}⌝"$'\n'"Error output:"$'\n'"⌜${content2}⌝"
  fi

}

function compareWithApproved() {
  local testDirectory exitCode approvedFile receivedFile receivedFileToCopy
  testDirectory="${1}"
  receivedFileToCopy="${2}"
  testName="${testDirectory##*/}"

  approvedFile="${testDirectory}/results.approved.md"
  receivedFile="${testDirectory}/results.received.md"

  if [[ ! -f "${approvedFile}" ]]; then
    log::debug "🧪 ${testName}: no approved file, creating one."
    : >"${approvedFile}"
  fi

  if diff --color -u "${approvedFile}" "${receivedFileToCopy}" 1>&2; then
    log::debug "🧪 ${testName}: OK, equal to approved file."
    if [[ -f "${receivedFile}" ]]; then
      rm -f "${receivedFile}" 1>/dev/null
    fi
    return 0
  else
    log::printString "${receivedFile} is different than ${approvedFile} (see above)"
    log::error "🧪 ${testName}: KO, differs from approved file (see difference above)."
  fi

  # if the option is activated, we approve the received file
  if [[ ${TEST_AUTO_APPROVE:-false} == "true" ]]; then
    log::info "🧪 ${testName}: Auto-approving"
    cp -f "${receivedFileToCopy}" "${approvedFile}"
    if [[ -f "${receivedFile}" ]]; then
      rm -f "${receivedFile}" 1>/dev/null
    fi
  else
    cp -f "${receivedFileToCopy}" "${receivedFile}"
  fi

  return 1
}

# Allows to explicitly warn the user that a test made Valet exit (it should not).
# Give guidance on what to do to fix this.
function onExitTestInternal() {
  local rc="${1:-}"

  resetFdRedirection
  resetOriginalImportantVariables

  if [[ -s "${_TEST_REPORT_FILE}" ]]; then
    log::errorTrace "Current test report:"
    log::printFile "${_TEST_REPORT_FILE}"
  else
    log::errorTrace "No test report yet for the current test."
  fi

  if [[ -s "${_TEST_STANDARD_OUTPUT_FILE}" ]]; then
    log::errorTrace "Current test standard output:"
    io::readFile "${_TEST_STANDARD_OUTPUT_FILE}"
    log::printFileString "${RETURNED_VALUE}"
  else
    log::errorTrace "No standard output for the current test."
  fi

  if [[ -s "${_TEST_STANDARD_ERROR_FILE}" ]]; then
    log::errorTrace "Current test error output:"
    io::readFile "${_TEST_STANDARD_ERROR_FILE}"
    log::printFileString "${RETURNED_VALUE}"
  else
    log::errorTrace "No error output for the current test."
  fi

  log::error "The program has exit during a test with code ${rc}. If you expect the tested function/program to exit or fail, then run it in a subshell and capture the exit code like that:"$'\n'"(myFunctionThatFails) || exitCode=\$?"

  GLOBAL_ERROR_DISPLAYED=1
}

# After this function call, everything written to stdout and stderr will be redirected
# to the test output files.
# Call resetFdRedirection to reset the redirection.
function setFdRedirection() {
  # redirect the standard output and error output to files
  exec 3>&1 1>"${_TEST_STANDARD_OUTPUT_FILE}"
  exec 4>&2 2>"${_TEST_STANDARD_ERROR_FILE}"

  # sets up a simpler log function for the tests
  # so we can have consistent results independent of the environment
  log::setLevel info true
  eval "${SIMPLIFIED_IMPORTANT_VARIABLES}"
}

# Restores the standard output and error output.
# Call this function after setFdRedirection.
function resetFdRedirection() {
  # reset the standard output and error output
  exec 1>&3 3>&-
  exec 2>&4 4>&-

  # reset the original logs
  log::setLevel "${ORIGINAL_GLOBAL_LOG_LEVEL}" true
  eval "${ORIGINAL_GLOBAL_LOG_PRINT_FUNCTION}"
}

# This function returns a string that can be evaluated to reset all
# valet config and global variables to their original values.
# As well as the log line function.
function setOriginalImportantVariables() {
  io::invoke declare -p ${!VALET_CONFIG_*} ${!GLOBAL_*}
  ORIGINAL_IMPORTANT_VARIABLES="${GLOBAL_LOG_PRINT_FUNCTION}"$'\n'"${RETURNED_VALUE//declare -? /}"
}

function resetOriginalImportantVariables() {
  unset -v ${!VALET_CONFIG_*} ${!GLOBAL_*}
  eval "${ORIGINAL_IMPORTANT_VARIABLES}"
}

# This function returns a string that can be evaluated to set all
# valet config and global variables to simpler values
# in order to have consistent results in the tests.
# As well as a simplified log line function.
function setSimplifiedImportantVariables() {
  # unset all VALET_ variables to not have the user variables interfere with the tests
  SIMPLIFIED_IMPORTANT_VARIABLES="
  unset -v ${!VALET_*}

  export VALET_CONFIG_ENABLE_COLORS=false
  export VALET_CONFIG_ENABLE_NERDFONT_ICONS=false
  export VALET_CONFIG_DISABLE_LOG_TIME=true
  export VALET_CONFIG_DISABLE_LOG_WRAP=true
  export VALET_CONFIG_ENABLE_LOG_TIMESTAMP=false
  export VALET_CONFIG_LOG_COLUMNS=9999

  export VALET_CONFIG_ICON_ERROR=IE
  export VALET_CONFIG_ICON_WARNING=IW
  export VALET_CONFIG_ICON_SUCCESS=IS
  export VALET_CONFIG_ICON_INFO=II
  export VALET_CONFIG_ICON_DEBUG=ID
  export VALET_CONFIG_ICON_EXIT=IX
  export VALET_CONFIG_ICON_STOPPED=IP
  export VALET_CONFIG_ICON_KILLED=IK

  export VALET_CONFIG_COLOR_DEFAULT=CDE
  export VALET_CONFIG_COLOR_DEBUG=CDB
  export VALET_CONFIG_COLOR_INFO=CIN
  export VALET_CONFIG_COLOR_WARNING=CWA
  export VALET_CONFIG_COLOR_SUCCESS=CSU
  export VALET_CONFIG_COLOR_ERROR=CER
  export VALET_CONFIG_COLOR_TIMESTAMP=CTI
  export VALET_CONFIG_COLOR_HIGHLIGHT=CHI
  export VALET_CONFIG_COLOR_TITLE=CTT
  export VALET_CONFIG_COLOR_OPTION=COP
  export VALET_CONFIG_COLOR_ARGUMENT=CAR
  export VALET_CONFIG_COLOR_COMMAND=CCO
  export VALET_CONFIG_COLOR_ACTIVE_BUTTON=CAB
  export VALET_CONFIG_COLOR_UNACTIVE_BUTTON=CUB

  export VALET_CONFIG_DOT_ENV_SCRIPT=false

  export GLOBAL_COLUMNS=9999

  export GLOBAL_TEST_TEMP_FILE=\"${_TEST_TEMP_FILE}\"
  "

  eval "${SIMPLIFIED_IMPORTANT_VARIABLES}"
  log::createPrintFunction
  SIMPLIFIED_IMPORTANT_VARIABLES+="
  ${GLOBAL_LOG_PRINT_FUNCTION}
  "
}

# This function is used to print the content of a file with some substitutions.
# The substitutions are:
# - replace the GLOBAL_VALET_HOME with $GLOBAL_VALET_HOME
# - replace the current test directory with a dot
# - replace a line with ${TMPDIR}/valet-*/ (temp directory) by /valet/
#
# This allows to have consistent results accross different execution environments for the tests.
#
# Usage:
#   myCommandThatProducesLinesWithValetDirectoryPath 2> "${GLOBAL_TEST_TEMP_FILE}"
#   echoFileSubstitutingPath "${GLOBAL_TEST_TEMP_FILE}" 1>&2
function echoFileSubstitutingPath() {
  local file="${1}"
  local line
  local IFS=$'\n'
  local -i firstLine=1
  while read -rd $'\n' line || [[ -n ${line:-} ]]; do
    if [[ firstLine -eq 1 ]]; then
      firstLine=0
    else
      echo
    fi
    line="${line//${GLOBAL_VALET_HOME}/\$GLOBAL_VALET_HOME}"
    line="${line//${CURRENT_DIRECTORY}/.}"
    line="${line//${GLOBAL_TEMPORARY_PREFIX}*.valet/\/tmp/valet}"
    line="${line//${GLOBAL_TEMPORARY_IN_MEM_PREFIX}*.valet/\/tmp/valet}"
    if [[ ${line} =~ "after "[0-9]{1,}s.$ ]]; then
      line="${line/%after */after Xs.}"
    fi
    printf '%s' "${line}"
  done <"${file}"
}
