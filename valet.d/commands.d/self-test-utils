#!/usr/bin/env bash
set -Eeu -o pipefail
# author: github.com/jcaillon
# description: Utility functions for the self-test script.

#===============================================================
# >>> Functions that can be used in the test scripts
#===============================================================

# shellcheck source=../lib-io
source io

# Call this function to add a paragraph in the report file.
#
# $1: the text to add in the report file
#
# Usage:
#   commentTest "This is a comment."
function commentTest() {
  printf "%s\n\n" "${1:-}" >>"${_TEST_REPORT_FILE}"
}

# Call this function after each test to write the test results to the report file.
# This create a new H3 section in the report file with the test description and the exit code.
#
# $1: the title of the test
# $2: the exit code of the test
# $3: (optional) a text to explain what is being tested
#
# Usage:
#   endTest "Testing something" $?
function endTest() {
  local testTitle="${1:-}"
  local exitCode="${2:-}"
  local testDescription="${3:-}"

  local fdRedirectionReset=false
  if [[ ${ORIGINAL_GLOBAL_LOG_LEVEL} == "debug" ]]; then
    resetFdRedirection
    fdRedirectionReset=true
    log::debug "Ended test ⌜${testTitle}⌝ with exit code ⌜${exitCode}⌝."
  fi

  {
    # write the test title
    printf "%s\n\n" "### ${testTitle:-test}"

    # write the test description if any
    if [[ -n "${testDescription}" ]]; then
      printf "%s\n\n" "${testDescription}"
    fi

    # write the exit code
    printf "%s\n\n" "Exit code: \`${exitCode}\`"

    # write the standard output if any
    if [[ -s "${_TEST_STANDARD_OUTPUT_FILE}" ]]; then
      printf "%s\n\n%s\n" "**Standard** output:" "\`\`\`plaintext"
      echoFileSubstitutingPath "${_TEST_STANDARD_OUTPUT_FILE}"
      printf "\n%s\n\n" "\`\`\`"
    fi

    # write the error output if any
    if [[ -s "${_TEST_STANDARD_ERROR_FILE}" ]]; then
      printf "%s\n\n%s\n" "**Error** output:" "\`\`\`log"
      echoFileSubstitutingPath "${_TEST_STANDARD_ERROR_FILE}"
      printf "\n%s\n\n" "\`\`\`"
    fi

  } >>"${_TEST_REPORT_FILE}"

  # reset the standard output and error output files
  : >"${_TEST_STANDARD_OUTPUT_FILE}"
  : >"${_TEST_STANDARD_ERROR_FILE}"

  if [[ ${fdRedirectionReset} == "true" ]]; then
    setFdRedirection
  fi

  set +Eeu +o pipefail
}

# This function allows to set the temp files and directories
# numbers so we can have consistent results.
#
# $1: the new number for temp files and directories
#
# Usage:
#   setTempFilesNumber 3
function setTempFilesNumber() {
  TEMPORARY_FILE_NUMBER=${1}
  TEMPORARY_DIRECTORY_NUMBER=${1}
}

function echoFileWithLineNumberSubstitution() {
  local file="${1}"
  local line
  local IFS=$'\n'
  while read -rd $'\n' line; do
    if [[ ${line} =~ :[0-9]{1,4}$ ]]; then
      line="${line/%:[[:digit:]]/:XXX}"
      line="${line/%:[[:digit:]][[:digit:]]/:XXX}"
      line="${line/%:[[:digit:]][[:digit:]][[:digit:]]/:XXX}"
      line="${line/%:[[:digit:]][[:digit:]][[:digit:]][[:digit:]]/:XXX}"
    fi
    echo "${line}"
  done <"${file}"
}

#===============================================================
# >>> Internal tests functions
#===============================================================

# Allows to run the core tests of Valet (tests.d directory in the repo) as
# well as the examples (examples.d in the repo).
function runCoreTests() {
  if [[ ! -d "${GLOBAL_VALET_HOME}/tests.d" ]]; then
    core::fail "The valet core tests directory ⌜${GLOBAL_VALET_HOME}/tests.d⌝ does not exist, cannot run core tests."
  fi

  # we need to rebuild the commands for the core commands only
  rebuildCommands ""

  log::info "Running all test suites in directory ⌜${GLOBAL_VALET_HOME}/tests.d⌝."
  runTestSuites "${GLOBAL_VALET_HOME}/tests.d"

  # now we can also test the commands in examples.d if the directory is there
  if [[ ! -d "${GLOBAL_VALET_HOME}/examples.d" ]]; then
    log::warning "The valet examples directory ⌜${GLOBAL_VALET_HOME}/examples.d⌝ does not exist, cannot run the tests on the core examples."
  else
    # we need to rebuild the commands for the examples only
    rebuildCommands "${GLOBAL_VALET_HOME}/examples.d"

    log::info "Running all test suites in directory ⌜${GLOBAL_VALET_HOME}/examples.d⌝."
    runTestSuites "${GLOBAL_VALET_HOME}/examples.d/showcase/tests.d"
  fi

  # reload the orignal commands
  core::reloadUserCommands
}

function rebuildCommands() {
  local originalLogLevel
  log::getLevel && originalLogLevel="${LAST_RETURNED_VALUE}"
  log::setLevel warning true
  core::sourceForFunction selfBuild
  selfBuild --output "" --user-directory "${1:-}"
  log::setLevel "${originalLogLevel}" true
}

# Run all the test suites in the given directory.
# A test suite is a folder that contains a test.sh file.
# This test.sh allows to run multiple tests.
#
# $1: the directory containing the test suites
#
# Returns:
#   the number of test suites run (increments the global variable NB_TEST_SUITES)
#   the number of failed test suites (increments the global variable NB_FAILED_TEST_SUITES)
#
# Usage:
#   runTestSuites "${GLOBAL_VALET_HOME}/tests.d"
function runTestSuites() {
  local testsDirectory="${1}"

  # save the original important variables so we can restore them after the test suite
  log::getLevel
  ORIGINAL_GLOBAL_LOG_LEVEL="${LAST_RETURNED_VALUE}"
  ORIGINAL_GLOBAL_LOG_PRINT_FUNCTION="${GLOBAL_LOG_PRINT_FUNCTION}"
  setOriginalImportantVariables
  setSimplifiedImportantVariables
  resetOriginalImportantVariables

  io::createTempFile && _TEST_STANDARD_OUTPUT_FILE="${LAST_RETURNED_VALUE}"
  io::createTempFile && _TEST_STANDARD_ERROR_FILE="${LAST_RETURNED_VALUE}"
  io::createTempFile && _TEST_REPORT_FILE="${LAST_RETURNED_VALUE}"
  io::createTempFile && _TEST_TEMP_FILE="${LAST_RETURNED_VALUE}"

  local -i failedTestSuites nbTestSuites
  failedTestSuites=0
  nbTestSuites=0

  # for each test file in the test directory
  local testDirectory exitCode testDirectoryName testScript
  for testDirectory in "${testsDirectory}/"*; do
    testDirectoryName="${testDirectory##*/}"

    # skip if not a directory
    if [[ ! -d "${testDirectory}" ]]; then continue; fi

    # skip if the test directory does not match the include pattern
    if [[ -n "${TEST_INCLUDE_PATTERN:-}" && ! ("${testDirectoryName}" =~ ${TEST_INCLUDE_PATTERN}) ]]; then
      log::debug "Skipping test ⌜${testDirectoryName##*/}⌝ because it does not match the include pattern."
      continue
    fi
    # skip if the test directory matches the exclude pattern
    if [[ -n "${TEST_EXCLUDE_PATTERN:-}" && "${testDirectoryName}" =~ ${TEST_EXCLUDE_PATTERN} ]]; then
      log::debug "Skipping test ⌜${testDirectoryName##*/}⌝ because it matches the exclude pattern."
      continue
    fi

    log::info "Running test suite ⌜${testDirectory##*/}⌝."

    # write the test suite title
    printf "%s\n\n" "# Test suite ${testDirectory##*/}" >"${_TEST_REPORT_FILE}"

    # for each .sh script in the test directory, run the test
    for testScript in "${testDirectory}"/*.sh; do
      # skip if not a file
      if [[ ! -f "${testScript}" ]]; then continue; fi

      log::info "Running test       ├── ⌜${testScript##*/}⌝."

      runTest "${testDirectory}" "${testScript}"
    done

    exitCode=0
    compareWithApproved "${testDirectory}" "${_TEST_REPORT_FILE}" || exitCode=$?
    nbTestSuites+=1

    if [[ "${exitCode}" -eq 0 ]]; then
      log::success "Test suite ${testDirectory##*/} passed."
    else
      log::error "Test suite ${testDirectory##*/} failed."
      failedTestSuites+=1
    fi
  done

  NB_TEST_SUITES=$((NB_TEST_SUITES + nbTestSuites))
  NB_FAILED_TEST_SUITES=$((NB_FAILED_TEST_SUITES + failedTestSuites))
}

function runTest() {
  local testDirectory="${1}"
  local testScript="${2}"

  # set a simplified log print function to have consistent results in tests
  eval "${SIMPLIFIED_IMPORTANT_VARIABLES}"

  # redirect the standard output and error output to files
  setFdRedirection

  # make sure that, if we exit in the test, we catch that and explain to the user
  # why this should not happen
  eval "function onExitTest() { onExitTestInternal; }"

  # used in echoFileSubstitutingPath to replace this path with .
  CURRENT_DIRECTORY="${PWD}"

  # write the test script name
  local scriptName="${testScript##*/}"
  scriptName="${scriptName%.sh}"
  printf "%s\n\n" "## Test script ${scriptName}" >>"${_TEST_REPORT_FILE}"

  # run the test
  pushd "${testDirectory}" >/dev/null
  # shellcheck disable=SC1091
  set +Eeu +o pipefail
  # shellcheck disable=SC1090
  source "${testScript}"
  set -Eeu -o pipefail
  popd >/dev/null

  # remove the onExitTest function, we can exit safely now
  unset -f onExitTest

  resetFdRedirection

  # restore the important variables
  resetOriginalImportantVariables

  # test if the user forgot to call endTest
  if [[ -s "${_TEST_STANDARD_OUTPUT_FILE}" || -s "${_TEST_STANDARD_ERROR_FILE}" ]]; then
    local content1 content2
    io::readFile "${_TEST_STANDARD_OUTPUT_FILE}" && content1="${LAST_RETURNED_VALUE}"
    io::readFile "${_TEST_STANDARD_ERROR_FILE}" && content2="${LAST_RETURNED_VALUE}"
    core::fail "The test script ⌜${testScript}⌝ did not call endTest OR it had outputs after the last endTest call."$'\n'"Standard output"$'\n'"⌜${content1}⌝"$'\n'"Error output:"$'\n'"⌜${content2}⌝"
  fi

}

function compareWithApproved() {
  local testDirectory exitCode approvedFile receivedFile receivedFileToCopy
  testDirectory="${1}"
  receivedFileToCopy="${2}"
  testName="${testDirectory##*/}"

  approvedFile="${testDirectory}/results.approved.md"
  receivedFile="${testDirectory}/results.received.md"

  if [[ ! -f "${approvedFile}" ]]; then
    log::debug "🧪 ${testName}: no approved file, creating one."
    : >"${approvedFile}"
  fi

  if diff --color -u "${approvedFile}" "${receivedFileToCopy}" 1>&2; then
    log::debug "🧪 ${testName}: OK, equal to approved file."
    rm -f "${receivedFile}" 2>/dev/null || :
    return 0
  else
    echo "${receivedFile} is different than ${approvedFile} (see above)" 1>&2
    log::error "🧪 ${testName}: KO, differs from approved file (see difference above)." | tee >(cat >&2)
  fi

  # if the option is activated, we approve the received file
  if [[ ${TEST_AUTO_APPROVE:-false} == "true" ]]; then
    log::info "🧪 ${testName}: Auto-approving"
    cp -f "${receivedFileToCopy}" "${approvedFile}"
    rm -f "${receivedFile}" 2>/dev/null || :
  else
    cp -f "${receivedFileToCopy}" "${receivedFile}"
  fi

  return 1
}

# Allows to explicitly warn the user that a test made Valet exit (it should not).
# Give guidance on what to do to fix this.
function onExitTestInternal() {
  local rc=$?

  resetFdRedirection

  log::getCallStack 4 && local stack="${LAST_RETURNED_VALUE}"

  local message="The program has exit during a test with code ${rc}, stack:${stack}"$'\n'
  if [[ -s "${_TEST_REPORT_FILE}" ]]; then
    io::readFile "${_TEST_REPORT_FILE}"
    message+="Current test report:"$'\n'"⌜${LAST_RETURNED_VALUE}⌝"$'\n'
  else
    message+="No test report yet for the current test."$'\n'
  fi
  if [[ -s "${_TEST_STANDARD_OUTPUT_FILE}" ]]; then
    io::readFile "${_TEST_STANDARD_OUTPUT_FILE}"
    message+="Current test standard output:"$'\n'"⌜${LAST_RETURNED_VALUE}⌝"$'\n'
  else
    message+="No standard output for the current test."$'\n'
  fi
  if [[ -s "${_TEST_STANDARD_ERROR_FILE}" ]]; then
    io::readFile "${_TEST_STANDARD_ERROR_FILE}"
    message+="Current test error output:"$'\n'"⌜${LAST_RETURNED_VALUE}⌝"$'\n'
  else
    message+="No error output for the current test."$'\n'
  fi
  message+="If you expect the tested function/program to exit/fail, then run it in a subshell like that:"$'\n'"(myFunctionThatFails)"

  log::error "${message}"

  GLOBAL_ERROR_DISPLAYED=1
}

# After this function call, everything written to stdout and stderr will be redirected
# to the test output files.
# Call resetFdRedirection to reset the redirection.
function setFdRedirection() {
  # redirect the standard output and error output to files
  exec 3>&1 1>"${_TEST_STANDARD_OUTPUT_FILE}"
  exec 4>&2 2>"${_TEST_STANDARD_ERROR_FILE}"

  # sets up a simpler log function for the tests
  # so we can have consistent results independent of the environment
  log::setLevel info true
  eval "${SIMPLIFIED_IMPORTANT_VARIABLES}"
}

# Restores the standard output and error output.
# Call this function after setFdRedirection.
function resetFdRedirection() {
  # reset the standard output and error output
  exec 1>&3 3>&-
  exec 2>&4 4>&-

  # reset the original logs
  log::setLevel "${ORIGINAL_GLOBAL_LOG_LEVEL}" true
  eval "${ORIGINAL_GLOBAL_LOG_PRINT_FUNCTION}"
}

# This function returns a string that can be evaluated to reset all
# valet config and global variables to their original values.
# As well as the log line function.
function setOriginalImportantVariables() {
  io::captureOutput declare -p ${!VALET_CONFIG_*} ${!GLOBAL_*}
  ORIGINAL_IMPORTANT_VARIABLES="${GLOBAL_LOG_PRINT_FUNCTION}"$'\n'"${LAST_RETURNED_VALUE//declare -? /}"
}

function resetOriginalImportantVariables() {
  unset -v ${!VALET_CONFIG_*} ${!GLOBAL_*}
  eval "${ORIGINAL_IMPORTANT_VARIABLES}"
}

# This function returns a string that can be evaluated to set all
# valet config and global variables to simpler values
# in order to have consistent results in the tests.
# As well as a simplified log line function.
function setSimplifiedImportantVariables() {
  SIMPLIFIED_IMPORTANT_VARIABLES="
  export VALET_CONFIG_ENABLE_COLORS=false
  export VALET_CONFIG_ENABLE_NERDFONT_ICONS=false
  export VALET_CONFIG_DISABLE_LOG_TIME=true
  export VALET_CONFIG_DISABLE_LOG_WRAP=true
  export VALET_CONFIG_ENABLE_LOG_TIMESTAMP=false
  export VALET_CONFIG_LOG_COLUMNS=9999

  export VALET_CONFIG_ICON_ERROR=IE
  export VALET_CONFIG_ICON_WARNING=IW
  export VALET_CONFIG_ICON_SUCCESS=IS
  export VALET_CONFIG_ICON_INFO=II
  export VALET_CONFIG_ICON_DEBUG=ID
  export VALET_CONFIG_ICON_EXIT=IX
  export VALET_CONFIG_ICON_STOPPED=IP
  export VALET_CONFIG_ICON_KILLED=IK

  export VALET_CONFIG_COLOR_DEFAULT=CDE
  export VALET_CONFIG_COLOR_DEBUG=CDB
  export VALET_CONFIG_COLOR_INFO=CIN
  export VALET_CONFIG_COLOR_WARNING=CWA
  export VALET_CONFIG_COLOR_SUCCESS=CSU
  export VALET_CONFIG_COLOR_ERROR=CER
  export VALET_CONFIG_COLOR_TIMESTAMP=CTI
  export VALET_CONFIG_COLOR_HIGHLIGHT=CHI
  export VALET_CONFIG_COLOR_TITLE=CTT
  export VALET_CONFIG_COLOR_OPTION=COP
  export VALET_CONFIG_COLOR_ARGUMENT=CAR
  export VALET_CONFIG_COLOR_COMMAND=CCO
  export VALET_CONFIG_COLOR_ACTIVE_BUTTON=CAB
  export VALET_CONFIG_COLOR_UNACTIVE_BUTTON=CUB

  export GLOBAL_COLUMNS=9999
  "
  eval "${SIMPLIFIED_IMPORTANT_VARIABLES}"
  log::createPrintFunction
  SIMPLIFIED_IMPORTANT_VARIABLES+="
  ${GLOBAL_LOG_PRINT_FUNCTION}
  "
}

# This function is used to echo the content of a file with some substitutions.
# The substitutions are:
# - replace the GLOBAL_VALET_HOME with $GLOBAL_VALET_HOME
# - replace the current test directory with a dot
# - replace a line with ${TMPDIR}/valet-*/ (temp directory) by /valet/
#
# This allows to have consistent results accross different execution environments for the tests.
#
# Usage:
#   myCommandThatProducesLinesWithValetDirectoryPath 2> "${_TEST_TEMP_FILE}"
#   echoFileSubstitutingPath "${_TEST_TEMP_FILE}" 1>&2
function echoFileSubstitutingPath() {
  local file="${1}"
  local line
  local IFS=$'\n'
  local -i firstLine=1
  while read -rd $'\n' line; do
    if [[ firstLine -eq 1 ]]; then
      firstLine=0
    else
      echo
    fi
    line="${line//${GLOBAL_VALET_HOME}/\$GLOBAL_VALET_HOME}"
    line="${line//${CURRENT_DIRECTORY}/.}"
    line="${line//${GLOBAL_TEMPORARY_PREFIX}*.valet/\/tmp/valet}"
    line="${line//${GLOBAL_TEMPORARY_IN_MEM_PREFIX}*.valet/\/tmp/valet}"
    if [[ ${line} =~ "after "[0-9]{1,}s.$ ]]; then
      line="${line/%after */after Xs.}"
    fi
    echo -n "${line}"
  done <"${file}"
}
