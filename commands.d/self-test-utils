#!/usr/bin/env bash
# author: github.com/jcaillon
# description: this script is a valet command

#===============================================================
# >>> Functions that can be used in the test scripts
#===============================================================

# shellcheck source=../libraries.d/lib-fs
source fs
# shellcheck source=../libraries.d/lib-test
source test
# shellcheck source=../libraries.d/lib-profiler
source profiler

# Rebuild all the commands silently.
function selfTestUtils_rebuildCommands() {
  local IFS=' '
  log::debug "Rebuilding the commands ⌜$*⌝."
  command::sourceFunction selfBuild
  selfBuild "$@" --silent
}

# Set the value of Valet variables to ensure consistency in the tests
# - to reset all valet config and global variables to their original values,
#   as well as the log line function.
# - valet config and global variables to simpler values
#   in order to have consistent results in the tests.
#   As well as a simplified log line function.
# shellcheck disable=SC2034
function selfTestUtils_setupValetForConsistency() {
  local IFS=' '
  # unset all VALET_ variables to not have the user variables interfere with the tests
  # shellcheck disable=SC2086
  unset -v ${!VALET_*}

  # same with all styles and escape codes
  # shellcheck disable=SC2086
  # we reset all the icon variables to a blank string
  local vars="${!STYLE_*} ${!ESC__*} ${!__ESC__*}"
  eval "${vars// /"=''; "}=''"

  # setup consistent logs
  export VALET_CONFIG_LOG_PATTERN="<level> <message>"
  export VALET_CONFIG_LOG_FORMATTED_EXTRA_EVAL=""
  export VALET_CONFIG_LOG_COLUMNS=9999
  export VALET_CONFIG_LOG_DISABLE_WRAP=true
  export VALET_CONFIG_LOG_DISABLE_HIGHLIGHT=true
  export VALET_CONFIG_LOG_FD=2
  export VALET_CONFIG_LOG_TO_DIRECTORY=""
  export VALET_CONFIG_LOG_FILENAME_PATTERN=""
  export GLOBAL_LOG_LEVEL_INT=1
  export GLOBAL_LOG_LEVEL=info
  log::init
  GLOBAL_PROGRAM_STARTED_AT_DIRECTORY="${PWD}"

  # consistent styles
  export VALET_CONFIG_ENABLE_COLORS=false
  export VALET_CONFIG_ENABLE_NERDFONT_ICONS=false
  export VALET_CONFIG_STYLE_SQUARED_BOXES=true
  export VALET_CONFIG_DISABLE_TEXT_ATTRIBUTES=true
  export VALET_CONFIG_DISABLE_ESC_CODES=true

  # other important variables
  export GLOBAL_COLUMNS=9999
  export VALET_CONFIG_DOT_ENV_SCRIPT=false
  export VALET_CONFIG_DISABLE_PROGRESS=true

  unset _PROGRESS_BAR_RUNNING

  # override time::getProgramElapsedMicroseconds to return a fake incremental time
  # shellcheck disable=SC2317
  function time::getProgramElapsedMicroseconds() {
    if [[ -z ${_FAKE_TIME:-} ]]; then
      _FAKE_TIME=0
      _TIME_FACTOR=1
    fi
    ((_FAKE_TIME=_FAKE_TIME+ 1000000 * _TIME_FACTOR, _TIME_FACTOR++))
    REPLY="${_FAKE_TIME}"
  }
}

# Set the value of important bash variables to ensure consistency in the tests.
# shellcheck disable=SC2034
function selfTestUtils_setupBashForConsistency() {
  # default IFS
  IFS=' '$'\t'$'\n'
  # fix the time to a known value
  export TZ=Etc/GMT+0
  unset EPOCHSECONDS
  export EPOCHSECONDS=548902800
  profiler::pause
  # because the profiler uses EPOCHREALTIME in PS4, we need to pause it first
  unset EPOCHREALTIME
  export EPOCHREALTIME=548902800.000000
  profiler::resume
  GLOBAL_TEST_ORIGINAL_BASHPID="${BASHPID}"
  unset BASHPID
  export BASHPID=1234
  unset SECONDS
  export SECONDS=0
  GLOBAL_TEST_ORIGINAL_OSTYPE="${OSTYPE}"
  export OSTYPE=linux-gnu
}

function selfTestUtils_runHookScript() {
  # run a custom script if it exists
  if [[ -f "${1}" ]]; then
    log::debug "Running (source) the script ⌜${1##*/}⌝."
    # shellcheck disable=SC1091
    # shellcheck disable=SC1090
    builtin source "${1}"
  else
    log::trace "No script found for ⌜${1##*/}⌝."
  fi
}

function selfTestUtils_compareWithApproved() {
  local testDirectory="${1}"
  local receivedFileToCopy="${2}"

  local exitCode approvedFile receivedFile

  approvedFile="${testDirectory}/results.approved.md"
  receivedFile="${testDirectory}/results.received.md"

  if [[ ! -f "${approvedFile}" ]]; then
    : >"${approvedFile}"
  fi

  local command="${GLOBAL_TEST_DIFF_COMMAND//%APPROVED_FILE%/"${approvedFile}"}"
  command="${command//%RECEIVED_FILE%/"${receivedFileToCopy}"}"

  if [[ -n ${GLOBAL_TEST_REPORT_FILE_MODE:-} ]]; then
    chmod "${GLOBAL_TEST_REPORT_FILE_MODE}" "${receivedFileToCopy}"
  fi

  if ${command} 1>&"${GLOBAL_FD_LOG}"; then
    log::success "→ test suite has passed."
    if [[ -f "${receivedFile}" ]]; then
      rm -f "${receivedFile}" 1>/dev/null
    fi
    return 0
  else
    log::printString "${receivedFile} is different than ${approvedFile}."
    log::error "→ test suite KO, received file differs from the approved file."
  fi

  # if the option is activated, we approve the received file
  if [[ ${_TEST_AUTO_APPROVE:-false} == "true" ]]; then
    log::info "→ test suite KO but auto-approving."
    cp -f "${receivedFileToCopy}" "${approvedFile}"
    if [[ -n ${GLOBAL_TEST_REPORT_FILE_MODE:-} ]]; then
      chmod "${GLOBAL_TEST_REPORT_FILE_MODE}" "${approvedFile}"
    fi
    if [[ -f "${receivedFile}" ]]; then
      rm -f "${receivedFile}" 1>/dev/null
    fi
  else
    cp -f "${receivedFileToCopy}" "${receivedFile}"
    if [[ -n ${GLOBAL_TEST_REPORT_FILE_MODE:-} ]]; then
      chmod "${GLOBAL_TEST_REPORT_FILE_MODE}" "${approvedFile}"
    fi
  fi

  return 1
}

function selfTestUtils_displayTestLogs() {
  if [[ -s "${GLOBAL_TEST_LOG_FILE}" ]]; then
    log::errorTrace "Logs for script ⌜${GLOBAL_TEST_SUITE_SCRIPT_NAME}⌝ (>&4):"
    log::printFile "${GLOBAL_TEST_LOG_FILE}"
  else
    log::errorTrace "Empty logs for script ⌜${GLOBAL_TEST_SUITE_SCRIPT_NAME}⌝."
  fi
}

function selfTestUtils_displayTestSuiteOutputs() {
  if [[ -s "${GLOBAL_TEST_REPORT_FILE}" ]]; then
    log::errorTrace "Test suite report for ⌜${GLOBAL_TEST_SUITE_NAME}⌝:"
    log::printFile "${GLOBAL_TEST_REPORT_FILE}"
  else
    log::errorTrace "Empty report for the test suite ⌜${GLOBAL_TEST_SUITE_NAME}⌝."
  fi

  if [[ -s "${GLOBAL_TEST_STANDARD_OUTPUT_FILE}" ]]; then
    log::errorTrace "Test suite standard output for ⌜${GLOBAL_TEST_SUITE_NAME}⌝:"
    log::printFile "${GLOBAL_TEST_STANDARD_OUTPUT_FILE}"
  else
    log::errorTrace "Empty standard output for the test suite ⌜${GLOBAL_TEST_SUITE_NAME}⌝."
  fi

  if [[ -s "${GLOBAL_TEST_STANDARD_ERROR_FILE}" ]]; then
    log::errorTrace "Test suite error output for ⌜${GLOBAL_TEST_SUITE_NAME}⌝:"
    log::printFile "${GLOBAL_TEST_STANDARD_ERROR_FILE}"
  else
    log::errorTrace "Empty error output for the test suite ⌜${GLOBAL_TEST_SUITE_NAME}⌝."
  fi
}

function selfTestUtils_setupDiffCommand() {
  GLOBAL_TEST_DIFF_COMMAND="${VALET_CONFIG_TEST_DIFF_COMMAND:-}"
  if [[ -z ${GLOBAL_TEST_DIFF_COMMAND} ]]; then
    if command -v delta &>/dev/null; then
      log::debug "Using delta as diff command."
      GLOBAL_TEST_DIFF_COMMAND="delta --paging=never --no-gitconfig --line-numbers --width ${GLOBAL_COLUMNS} --side-by-side %APPROVED_FILE% %RECEIVED_FILE%"
      # delta compares the file modes, so we need to match them
      GLOBAL_TEST_REPORT_FILE_MODE="${VALET_CONFIG_TEST_REPORT_FILE_MODE:-644}"
    elif command -v diff &>/dev/null; then
      log::debug "Using diff as diff command."
      GLOBAL_TEST_DIFF_COMMAND="diff --color -u %APPROVED_FILE% %RECEIVED_FILE%"
    elif command -v cmp &>/dev/null; then
      log::warning "Using cmp as diff command, consider setting up a diff tool using the VALET_CONFIG_TEST_DIFF_COMMAND config variable."
      GLOBAL_TEST_DIFF_COMMAND="cmp %APPROVED_FILE% %RECEIVED_FILE%"
    else
      log::warning "Using internal comparison function, consider setting up a diff tool using the VALET_CONFIG_TEST_DIFF_COMMAND config variable."
      GLOBAL_TEST_DIFF_COMMAND="selfTestUtils_internalCompare %APPROVED_FILE% %RECEIVED_FILE%"
    fi
  else
    string::getField GLOBAL_TEST_DIFF_COMMAND 0 separator=" "
    local diffExecutable="${REPLY}"
    if ! command -v "${diffExecutable}" &>/dev/null; then
      log::warning "The user diff command ⌜${diffExecutable}⌝ set with VALET_CONFIG_TEST_DIFF_COMMAND is not available, using the internal comparison function."
    fi
  fi
}

# This function allows to compare the content two files.
# It mimics the diff command.
#
# - $1: the first file to compare
# - $2: the second file to compare
#
# Returns:
#
# -$?: 0 if the files are the same, 1 otherwise
#
# ```bash
# selfTestUtils_internalCompare file1 file2
# ```
#
#TODO: print a better display of the differences
function selfTestUtils_internalCompare() {
  local file1="${1}"
  local file2="${2}"

  local content1 content2
  fs::readFile "${file1}"
  content1="${REPLY}"
  fs::readFile "${file2}"
  content2="${REPLY}"

  if [[ "${content1}" != "${content2}" ]]; then
    return 1
  fi
  return 0
}

# ## selfTestUtils_makeReplacementsInReport
#
# In order to have consistent results in tests, we need to replace to replace
# values that could be different on each run/machine.
function selfTestUtils_makeReplacementsInReport() {
  fs::readFile "${GLOBAL_TEST_REPORT_FILE}"
  REPLY="${REPLY//${GLOBAL_INSTALLATION_DIRECTORY}\/valet/valet}"
  REPLY="${REPLY//${GLOBAL_INSTALLATION_DIRECTORY}/\$GLOBAL_INSTALLATION_DIRECTORY}"
  REPLY="${REPLY//${GLOBAL_TEST_SUITE_DIRECTORY}/.}"
  REPLY="${REPLY//${GLOBAL_TEMPORARY_DIRECTORY_PREFIX}/\/tmp/valet}"
  REPLY="${REPLY//${GLOBAL_TEMPORARY_FILE_PREFIX}/\/tmp/valet}"
  printf "%s" "${REPLY}" >"${GLOBAL_TEST_REPORT_FILE}"
}

# ## selfTestUtils_prepareModifiedTrapFunctions
#
# Prepare modified versions of trap functions that can be used inside the test coproc
# and which allow to capture the stack trace on exit or error.
function selfTestUtils_prepareModifiedTrapFunctions() {
  bash::getBuiltinOutput declare -f trap::onErrorInternal
  _TEST_TRAP_FUNCTIONS_ORIGINAL="${REPLY}"
  _TEST_TRAP_FUNCTIONS_MODIFIED="${_TEST_TRAP_FUNCTIONS_ORIGINAL/: before-exit/"selfTestUtils_onTestError"}"

  if [[ ${_TEST_TRAP_FUNCTIONS_ORIGINAL} != *": before-exit"* ]]; then
    core::fail "Expected to find the string ': before-exit' in the original trap::onErrorInternal function."
  fi

  bash::injectCodeInFunction trap::onSubshellExitInternal "selfTestUtils_onTestExit"
  _TEST_TRAP_FUNCTIONS_MODIFIED+=$'\n'"${REPLY}"
  _TEST_TRAP_FUNCTIONS_ORIGINAL+=$'\n'"${REPLY2}"

  # shellcheck disable=SC2016
  bash::injectCodeInFunction trap::registerSubshell 'eval "${_TEST_TRAP_FUNCTIONS_ORIGINAL}"'
  _TEST_TRAP_FUNCTIONS_MODIFIED+=$'\n'"${REPLY}"
  _TEST_TRAP_FUNCTIONS_ORIGINAL+=$'\n'"${REPLY2}"
}

function selfTestUtils_onTestError() {
  if [[ ! -s ${GLOBAL_TEST_STACK_FILE} ]]; then
    GLOBAL_LAST_BASH_COMMAND="${BASH_COMMAND:-}"
    GLOBAL_STACK_FUNCTION_NAMES_ERR=("${FUNCNAME[@]}")
    GLOBAL_STACK_SOURCE_FILES_ERR=("${BASH_SOURCE[@]}")
    GLOBAL_STACK_LINE_NUMBERS_ERR=("${BASH_LINENO[@]}")
    declare -p GLOBAL_LAST_BASH_COMMAND GLOBAL_STACK_FUNCTION_NAMES_ERR GLOBAL_STACK_SOURCE_FILES_ERR GLOBAL_STACK_LINE_NUMBERS_ERR >>"${GLOBAL_TEST_STACK_FILE}"
  fi
}

# setup extra functions that will allow us to capture the stack trace on exit or error in the test script
function selfTestUtils_onTestExit() {
  if [[ ! -s ${GLOBAL_TEST_STACK_FILE} ]]; then
    GLOBAL_LAST_BASH_COMMAND="${BASH_COMMAND:-}"
    GLOBAL_STACK_FUNCTION_NAMES=("${FUNCNAME[@]}")
    GLOBAL_STACK_SOURCE_FILES=("${BASH_SOURCE[@]}")
    GLOBAL_STACK_LINE_NUMBERS=("${BASH_LINENO[@]}")
    declare -p GLOBAL_LAST_BASH_COMMAND GLOBAL_STACK_FUNCTION_NAMES GLOBAL_STACK_SOURCE_FILES GLOBAL_STACK_LINE_NUMBERS >>"${GLOBAL_TEST_STACK_FILE}"
  fi
}